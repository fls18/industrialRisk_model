# -*- coding: utf-8 -*-
"""
ì‚°ì—… ì•ˆì „ ìœ„í—˜ë„ ì˜ˆì¸¡ - 'ê¸°íƒ€' ì ë¦¼ ê°•ì œ ë³´ì •íŒ (wide ë¹„ëŠ” ë¬¸ì œ í•´ê²°)
- 25ê°œ ì—‘ì…€ ìë™ ìŠ¤ìº”
- í—¤ë” ì¶”ì •/ë‘ì¤„í—¤ë”/ì¤‘ë³µì»¬ëŸ¼/í•©ê³„í–‰ ì œê±°/ìˆ«ìí™” ì•ˆì „ ì²˜ë¦¬
- ì—…ì¢…/ì§€ì—­ ìë™ ìŠ¹ê²©(ì§€ì—­ëª… ì‚¬ì „)
- â˜… ì¸¡ì •í•­ëª©ì´ ëª¨ë‘ 'ê¸°íƒ€'ì¼ ë•Œ íŒŒì¼ ì¢…ë¥˜ë¡œ ë¶„ë¥˜ ê°•ì œ(ê·¼ì†ê¸°ê°„â†’ê·¼ì†ë²„í‚· ë§¤í•‘)
- long â†’ wide â†’ ì„ì‹œ Risk_Score ìƒì„±
- nâ‰¥20ì¼ ë•Œë§Œ Keras í•™ìŠµ(ì•„ë‹ˆë©´ lookup ì €ì¥)
"""

import os, re, glob, json, pickle, warnings
import numpy as np
import pandas as pd
from typing import List, Tuple, Optional, Dict

warnings.filterwarnings("ignore", category=FutureWarning)

# â”€â”€ (ì„ íƒ) ìˆ˜ë™ ì˜¤ë²„ë¼ì´ë“œ â”€â”€
OVERRIDES: Dict[str, Dict] = {
    # "ì¬í•´ì •ë„_2019": {"header_row": 1, "sheet_name": 0, "ids": ["ì—…ì¢…","ì¬í•´ì •ë„"]},
}

PATTERNS = ["*ê·œëª¨*.xlsx","*ê·¼ì†ê¸°ê°„*.xlsx","*ë°œìƒí˜•íƒœ*.xlsx","*ì¬í•´ì •ë„*.xlsx","*ì§€ì—­*.xlsx"]
BAN_NAMES = {"industrial_risk_model.h5","scaler.pkl","industrial_clean.csv","industrial_risk.csv",
             "num_cols.json","onehot_meta.json","group_means.json","training_curve.png",
             "risk_lookup.csv","feature_lookup.csv"}

REGION_NAMES = {
    "ì„œìš¸","ë¶€ì‚°","ëŒ€êµ¬","ì¸ì²œ","ê´‘ì£¼","ëŒ€ì „","ìš¸ì‚°","ì„¸ì¢…",
    "ê²½ê¸°","ê°•ì›","ì¶©ë¶","ì¶©ë‚¨","ì „ë¶","ì „ë‚¨","ê²½ë¶","ê²½ë‚¨","ì œì£¼",
    "ì„œìš¸íŠ¹ë³„ì‹œ","ë¶€ì‚°ê´‘ì—­ì‹œ","ëŒ€êµ¬ê´‘ì—­ì‹œ","ì¸ì²œê´‘ì—­ì‹œ","ê´‘ì£¼ê´‘ì—­ì‹œ","ëŒ€ì „ê´‘ì—­ì‹œ","ìš¸ì‚°ê´‘ì—­ì‹œ",
    "ê²½ê¸°ë„","ê°•ì›ë„","ì¶©ì²­ë¶ë„","ì¶©ì²­ë‚¨ë„","ì „ë¼ë¶ë„","ì „ë¼ë‚¨ë„","ê²½ìƒë¶ë„","ê²½ìƒë‚¨ë„","ì œì£¼íŠ¹ë³„ìì¹˜ë„",
}

def list_all_files(patterns: List[str]) -> List[str]:
    files=[]
    for p in patterns: files += glob.glob(p)
    return sorted([f for f in files if os.path.basename(f) not in BAN_NAMES])

def extract_year_from_name(path: str) -> Optional[int]:
    m = re.search(r"(20\d{2})", os.path.basename(path))
    return int(m.group(1)) if m else None

def detect_kind_from_name(path: str) -> str:
    n = os.path.basename(path)
    if "ê·œëª¨" in n: return "ê·œëª¨"
    if "ê·¼ì†ê¸°ê°„" in n: return "ê·¼ì†ê¸°ê°„"
    if "ë°œìƒí˜•íƒœ" in n: return "ë°œìƒí˜•íƒœ"
    if "ì¬í•´ì •ë„" in n: return "ì¬í•´ì •ë„"
    if "ì§€ì—­" in n: return "ì§€ì—­"
    return "ê¸°íƒ€"

def _stripspaces(x: str) -> str:
    return re.sub(r"\s+", "", str(x).replace("\u00a0"," ").strip())

def normalize_cols(cols: list) -> list:
    out=[]
    for c in cols:
        cc = _stripspaces(c)
        cc = (cc.replace("ëŒ€ì—…ì¢…","ì—…ì¢…")
                .replace("ì—° ë„","ì—°ë„").replace("ì—°ë„ë„","ì—°ë„")
                .replace("ê·¼ì†(ë…„)","ê·¼ì†ê¸°ê°„")
                .replace("ê¹”ë¦¼/ë’¤ì§‘í˜","ê¹”ë¦¼.ë’¤ì§‘í˜").replace("ê¹”ë¦¼.ë’¤ì§‘í˜","ê¹”ë¦¼.ë’¤ì§‘í˜"))
        if cc in ("","None","nan"): cc = None
        out.append(cc)
    fixed=[]
    for i,h in enumerate(out):
        fixed.append(f"col_{i}" if (not h or (isinstance(h,str) and h.startswith("Unnamed"))) else h)
    return fixed

def _convert_series_to_numeric(s: pd.Series) -> pd.Series:
    return (s.astype(str)
             .str.replace(",","",regex=False)
             .str.replace("%","",regex=False)
             .str.replace("\u00a0","",regex=False)
             .str.replace("âˆ’","-",regex=False)
             .str.strip()
             .replace({"":"0","-":"0","nan":"0","None":"0"})
             .pipe(pd.to_numeric, errors="coerce")
             .fillna(0))

def safe_to_numeric(df: pd.DataFrame, cols: list) -> pd.DataFrame:
    for c in cols:
        if c not in df.columns: 
            continue
        target = df.loc[:, c]  # ì¤‘ë³µëª…ì¼ ìˆ˜ ìˆìŒ
        if isinstance(target, pd.DataFrame):
            for sub in target.columns:
                df[sub] = _convert_series_to_numeric(df[sub])
        else:
            df[c] = _convert_series_to_numeric(target)
    return df

KNOWN_EVENT_COLS = {
    "ë–¨ì–´ì§","ë„˜ì–´ì§","ë¶€ë”ªí˜","ë¬¼ì²´ì—ë§ìŒ","ë¬´ë„ˆì§","ë¼ì„","ì ˆë‹¨ë² ì„ì°”ë¦¼","ê°ì „",
    "í­ë°œíŒŒì—´","í™”ì¬","ê¹”ë¦¼.ë’¤ì§‘í˜","ì´ìƒì˜¨ë„ë¬¼ì²´ì ‘ì´‰","ë¹ ì§ìµì‚¬","ë¶ˆê· í˜•ë°ë¬´ë¦¬í•œë™ì‘",
    "ì‚¬ì—…ì¥ì™¸êµí†µì‚¬ê³ ","ì—…ë¬´ìƒì§ˆë³‘","ì²´ìœ¡í–‰ì‚¬","í­ë ¥í–‰ìœ„","ë™ë¬¼ìƒí•´","ê¸°íƒ€","ë¶„ë¥˜ë¶ˆëŠ¥",
    "ê¹”ë¦¼","ë’¤ì§‘í˜"
}
def _looks_scale_bucket(name: str) -> bool:
    x = str(name)
    return bool(re.search(r"(\d+\s*~\s*\d+ì¸|\d+\s*-\s*\d+ì¸|\d+ì¸ë¯¸ë§Œ|\d+ì¸ì´ìƒ|\d+ì¸|ì†Œê·œëª¨|ì¤‘ê·œëª¨|ëŒ€ê·œëª¨)", x))

def _looks_tenure_bucket(name: str) -> bool:
    x = str(name)
    return bool(re.search(r"(P_0_1|P_2_3|P_4_5|P_6_10|P_11p|\d+ë…„ë¯¸ë§Œ|\d+\s*~\s*\d+ë…„|\d+ë…„ì´ìƒ)", x))

def classify_dim(colname: str) -> str:
    x = str(colname)
    if x in KNOWN_EVENT_COLS: return "ë°œìƒí˜•íƒœ"
    if _looks_scale_bucket(x): return "ê·œëª¨"
    if _looks_tenure_bucket(x): return "ê·¼ì†ë²„í‚·"
    if re.search(r"(ì‚¬ë§|ì¤‘ìƒ|ê²½ìƒ|íœ´ì—…|ì¬í•´ì •ë„)", x): return "ì¬í•´ì •ë„"
    return "ê¸°íƒ€"

def apply_override(path: str):
    b = os.path.basename(path)
    for k,cfg in OVERRIDES.items():
        if k in b: return cfg
    return {}

def read_excel_any(path: str, sheet_name=None) -> pd.DataFrame:
    target = 0 if sheet_name is None else sheet_name
    try:
        df = pd.read_excel(path, sheet_name=target, header=None)
    except Exception:
        df = pd.read_excel(path, sheet_name=target, header=None, engine="xlrd")
    if isinstance(df, dict):
        for _,sdf in df.items():
            if isinstance(sdf,pd.DataFrame) and not sdf.dropna(how="all").empty and sdf.shape[1]>0:
                return sdf
        return next(iter(df.values()))
    return df

def guess_header_row(df: pd.DataFrame) -> int:
    tokens = {"ì—…ì¢…","ì§€ì—­","ê·œëª¨","ê·¼ì†ê¸°ê°„","ë°œìƒí˜•íƒœ","ì¬í•´ì •ë„","ì—°ë„","í•©ê³„","ê³„","ì´ê³„"}
    best_row, best_score = 0, -1e9
    for r in range(min(6,len(df))):
        vals = df.iloc[r].astype(str).tolist()
        n_nonempty = sum(v.strip() not in ("","nan","None") for v in vals)
        if n_nonempty==0: continue
        score=0
        for v in vals:
            v2 = _stripspaces(v)
            if v2 in tokens: score+=4
            if v2 in KNOWN_EVENT_COLS: score+=2
            if v2.startswith("Unnamed"): score-=1
        score += n_nonempty*0.1
        if score>best_score: best_score, best_row = score, r
    return best_row

def repair_duplicate_headers(body: pd.DataFrame) -> pd.DataFrame:
    cols = list(body.columns)
    dup = body.columns.duplicated(keep=False).any() or any(re.match(r".+\.\d+$", c) for c in cols)
    if dup and any(c.startswith("ì—…ì¢…") for c in cols):
        first = body.iloc[0].astype(str).str.replace("\u00a0","",regex=False).str.strip()
        new_cols=[]
        for c in cols:
            if c.startswith("ì—…ì¢…") and str(first.get(c,"")) not in ("","nan","None"):
                new_cols.append(_stripspaces(first[c]))
            else:
                new_cols.append(c)
        body = body.copy()
        body.columns = normalize_cols(new_cols)
        body = body.iloc[1:].reset_index(drop=True)
    return body

def _try_promote_nextrow_as_header(body: pd.DataFrame) -> Tuple[pd.DataFrame, bool]:
    if body.empty: return body, False
    first_row = body.iloc[0]
    num_ratio = pd.to_numeric(first_row, errors="coerce").notna().mean()
    if num_ratio >= 0.7:
        new_cols = normalize_cols(first_row.tolist())
        body2 = body.iloc[1:].reset_index(drop=True).copy()
        if len(new_cols) >= body2.shape[1]:
            body2.columns = new_cols[:body2.shape[1]]
            return body2, True
    return body, False

def infer_header_and_fix(df: pd.DataFrame, force_header_row: Optional[int]=None) -> Tuple[pd.DataFrame,list,int]:
    hr = guess_header_row(df) if force_header_row is None else int(force_header_row)
    header = df.iloc[hr].astype(str).tolist()
    body = df.iloc[hr+1:].reset_index(drop=True).copy()
    body = body.dropna(axis=1, how="all")
    cols = normalize_cols(header[:len(body.columns)])
    body.columns = cols
    body = body.dropna(how="all").reset_index(drop=True)
    body = repair_duplicate_headers(body)
    body, _ = _try_promote_nextrow_as_header(body)
    return body, list(body.columns), hr

def _series_of(df: pd.DataFrame, colname: str) -> pd.Series:
    obj = df.loc[:, colname]
    if isinstance(obj, pd.DataFrame):
        return obj.iloc[:, 0]
    return obj

def _uniq_value_cols(body: pd.DataFrame, value_cols: list, ids: list) -> list:
    seen = {}
    new_cols = []
    for c in value_cols:
        if c in ids: 
            continue
        name = c
        while (body.columns == name).sum() > 1 or name in seen:
            k = seen.get(c, 0) + 1
            name = f"{c}__dup{k}"
            seen[c] = k
        if name != c:
            idx = list(body.columns).index(c)
            cols = list(body.columns); cols[idx] = name
            body.columns = cols
        new_cols.append(name)
    return new_cols

def _maybe_promote_id_columns(body: pd.DataFrame, ids: list) -> list:
    if "ì—…ì¢…" in ids and "ì§€ì—­" in ids: 
        return ids[:]
    candidates = []
    for c in body.columns:
        if c in ids: 
            continue
        s = _series_of(body, c)
        if s.isna().all(): 
            continue
        if pd.to_numeric(s, errors="coerce").notna().mean() >= 0.85:
            continue
        nunq = s.astype(str).str.strip().replace({"": "NaN"}).nunique(dropna=True)
        if 2 <= nunq <= 200:
            candidates.append(c)
    region_best = None; region_hit = 0
    for c in candidates:
        s = _series_of(body, c)
        vals = set(s.astype(str).str.strip())
        hit = len(vals & REGION_NAMES)
        if hit > region_hit:
            region_hit, region_best = hit, c
    new_ids = ids[:]
    if region_best and "ì§€ì—­" not in new_ids:
        body.rename(columns={region_best:"ì§€ì—­"}, inplace=True, errors="ignore")
        new_ids.append("ì§€ì—­")
    if "ì—…ì¢…" not in new_ids:
        for c in candidates:
            if c == region_best: 
                continue
            body.rename(columns={c:"ì—…ì¢…"}, inplace=True, errors="ignore")
            new_ids.append("ì—…ì¢…")
            break
    return list(dict.fromkeys(new_ids))

# â”€â”€ íŒŒì¼â†’long â”€â”€
def melt_one_file(path: str) -> pd.DataFrame:
    cfg = apply_override(path)
    raw = read_excel_any(path, sheet_name=cfg.get("sheet_name", None))
    if "header_row" in cfg:
        body, cols, used_header_row = infer_header_and_fix(raw, force_header_row=int(cfg["header_row"]))
    else:
        body, cols, used_header_row = infer_header_and_fix(raw, force_header_row=None)

    body["íŒŒì¼"] = os.path.basename(path)
    kind = detect_kind_from_name(path)

    id_candidates = [c for c in ["ì—…ì¢…","ì§€ì—­","ê·œëª¨","ê·¼ì†ê¸°ê°„","ë°œìƒí˜•íƒœ","ì¬í•´ì •ë„","ì—°ë„","íŒŒì¼"] if c in body.columns]
    prefer_ids = {
        "ê·œëª¨":["ì—…ì¢…","íŒŒì¼"],
        "ê·¼ì†ê¸°ê°„":["ì—…ì¢…","íŒŒì¼"],
        "ë°œìƒí˜•íƒœ":["ì—…ì¢…","íŒŒì¼"],
        "ì¬í•´ì •ë„":["ì—…ì¢…","íŒŒì¼"],
        "ì§€ì—­":["ì—…ì¢…","ì§€ì—­","íŒŒì¼"],
    }.get(kind, ["íŒŒì¼"])

    if "ids" in cfg:
        ids = [c for c in cfg["ids"] if c in body.columns]
    else:
        ids = [c for c in prefer_ids if c in body.columns]
        if not ids:
            ids = id_candidates[:2] if len(id_candidates)>=2 else id_candidates

    # í•©ê³„/ê³„/ì´ê³„ í–‰ ì œê±°
    for k in ["ì—…ì¢…","ì§€ì—­","ê·œëª¨","ê·¼ì†ê¸°ê°„","ë°œìƒí˜•íƒœ","ì¬í•´ì •ë„"]:
        if k in body.columns:
            s = _series_of(body, k)
            body[k] = s.astype(str).str.replace("\u00a0","",regex=False).str.strip()
            body = body[~body[k].astype(str).str.contains(r"(í•©ê³„|ì´ê³„|^ê³„$)", na=False)]

    # ê°’ ì»¬ëŸ¼
    value_cols = [c for c in body.columns if c not in ids]
    drop_sum_cols = [c for c in value_cols if re.search(r"(í•©ê³„|ì´ê³„|^ê³„$)", str(c))]
    if drop_sum_cols:
        body = body.drop(columns=drop_sum_cols, errors="ignore")
    value_cols = [c for c in value_cols if c not in drop_sum_cols]

    # ì—…ì¢…/ì§€ì—­ ìë™ ìŠ¹ê²©
    ids = _maybe_promote_id_columns(body, ids)
    value_cols = [c for c in body.columns if c not in ids]

    # ê°’ ì»¬ëŸ¼ ì¤‘ë³µëª… ìœ ë‹ˆí¬í™” + ìˆ«ìí™”
    value_cols = _uniq_value_cols(body, value_cols, ids)
    body = safe_to_numeric(body, value_cols)

    long_df = body.melt(id_vars=[c for c in ids if c in body.columns],
                        value_vars=value_cols,
                        var_name="ì¸¡ì •í•­ëª©", value_name="ê°’")
    long_df = long_df[~long_df["ì¸¡ì •í•­ëª©"].astype(str).str.contains(r"(í•©ê³„|ì´ê³„|^ê³„$)", na=False)]

    # ë¶„ë¥˜
    long_df["ì¸¡ì •í•­ëª©"] = long_df["ì¸¡ì •í•­ëª©"].astype(str).map(_stripspaces)
    long_df["ë¶„ë¥˜"] = long_df["ì¸¡ì •í•­ëª©"].apply(classify_dim)

    # â˜… ì „ë¶€ 'ê¸°íƒ€'ì¸ ê²½ìš°: íŒŒì¼ ì¢…ë¥˜ë¡œ ê°•ì œ ë³´ì •
    if long_df["ë¶„ë¥˜"].eq("ê¸°íƒ€").all():
        forced = "ê·¼ì†ë²„í‚·" if kind=="ê·¼ì†ê¸°ê°„" else kind
        if forced in {"ê·œëª¨","ê·¼ì†ë²„í‚·","ë°œìƒí˜•íƒœ","ì¬í•´ì •ë„"}:
            long_df["ë¶„ë¥˜"] = forced

    long_df["ì—°ë„"] = extract_year_from_name(path) or 0
    long_df["íŒŒì¼"] = os.path.basename(path)
    long_df["ì¢…ë¥˜"] = kind
    long_df["__í—¤ë”í–‰"] = used_header_row

    for c in ids + ["ì¸¡ì •í•­ëª©","ë¶„ë¥˜"]:
        if c in long_df.columns:
            long_df[c] = long_df[c].astype(str).str.replace("\u00a0","",regex=False).str.strip()
    return long_df

# â”€â”€ ë¡œë“œ â†’ long â”€â”€
FILES = list_all_files(PATTERNS)
if not FILES:
    raise SystemExit("âŒ .xlsx íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (íŒŒì¼ëª…ì— ê·œëª¨/ê·¼ì†ê¸°ê°„/ë°œìƒí˜•íƒœ/ì¬í•´ì •ë„/ì§€ì—­ í¬í•¨)")

print("ğŸ“‚ ìŠ¤ìº” ëŒ€ìƒ:")
for f in FILES: print(" -", f)

all_long = pd.concat([melt_one_file(p) for p in FILES], ignore_index=True)
for c in ["ì—…ì¢…","ì§€ì—­","ê·œëª¨","ê·¼ì†ê¸°ê°„","ë°œìƒí˜•íƒœ","ì¬í•´ì •ë„","ì¸¡ì •í•­ëª©","ë¶„ë¥˜","íŒŒì¼"]:
    if c in all_long.columns:
        all_long[c] = all_long[c].astype(str).str.strip().replace({"nan":"", "None":""})

print("ğŸ“ long shape:", all_long.shape)
print("ğŸ” ì˜ˆì‹œ:\n", all_long.head(5))
print("ğŸ“Š ë¶„ë¥˜ ì¹´ìš´íŠ¸:\n", all_long["ë¶„ë¥˜"].value_counts(dropna=False).to_dict())

# â”€â”€ long â†’ wide & ì„ì‹œ ë¼ë²¨ â”€â”€
SEV_W = {"ì‚¬ë§":1.0,"ì‚¬ë§ì":1.0,"ì¤‘ìƒ":0.7,"ì¤‘ìƒì":0.7,"ê²½ìƒ":0.4,"ê²½ìƒì":0.4,"íœ´ì—…":0.5,"íœ´ì—…ì¬í•´":0.5}

def build_wide(df: pd.DataFrame) -> pd.DataFrame:
    out=[]
    for kind in ["ë°œìƒí˜•íƒœ","ê·œëª¨","ê·¼ì†ë²„í‚·","ì¬í•´ì •ë„"]:
        sub = df[df["ë¶„ë¥˜"]==kind].copy()
        if sub.empty: 
            continue
        keys = [k for k in ["ì—…ì¢…","ì§€ì—­","ì—°ë„","íŒŒì¼"] if k in sub.columns]
        pvt = sub.pivot_table(index=keys, columns="ì¸¡ì •í•­ëª©", values="ê°’", aggfunc="sum", fill_value=0)
        pvt.columns = [f"{kind}:{_stripspaces(c)}" for c in pvt.columns]
        out.append(pvt)
    if not out: 
        return pd.DataFrame()
    wide = pd.concat(out, axis=1).reset_index()
    wide.columns = [_stripspaces(c) for c in wide.columns]
    if "ì—°ë„" not in wide.columns and "íŒŒì¼" in wide.columns:
        wide["ì—°ë„"] = wide["íŒŒì¼"].apply(lambda x: extract_year_from_name(x) or 0)
    return wide

wide = build_wide(all_long)
if wide.empty:
    raise SystemExit("âŒ ë³€í™˜ëœ wide í…Œì´ë¸”ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. (ê°•ì œ ë¶„ë¥˜ í›„ì—ë„ ë¹„ì •ìƒ) â€” OVERRIDESì—ì„œ header_row/idsë¥¼ ì§€ì •í•´ë³´ì„¸ìš”.")

scale_cols = [c for c in wide.columns if c.startswith("ê·œëª¨:")]
tenur_cols = [c for c in wide.columns if c.startswith("ê·¼ì†ë²„í‚·:")]
event_cols = [c for c in wide.columns if c.startswith("ë°œìƒí˜•íƒœ:")]
sev_cols   = [c for c in wide.columns if c.startswith("ì¬í•´ì •ë„:")]

wide["ì‚¬ê±´ì´í•©"] = 0.0
if event_cols:   wide["ì‚¬ê±´ì´í•©"] = wide[event_cols].sum(axis=1)
elif scale_cols: wide["ì‚¬ê±´ì´í•©"] = wide[scale_cols].sum(axis=1)
elif tenur_cols: wide["ì‚¬ê±´ì´í•©"] = wide[tenur_cols].sum(axis=1)

if sev_cols:
    def sev_sum(row):
        s=0.0
        for c in sev_cols:
            name = c.split(":",1)[1]
            w = SEV_W.get(name, 0.5)
            s += row[c]*w
        return s
    wide["ê°€ì¤‘ì‚¬ê±´"] = wide.apply(sev_sum, axis=1)
else:
    wide["ê°€ì¤‘ì‚¬ê±´"] = wide["ì‚¬ê±´ì´í•©"]

# ê·¸ë£¹í‚¤ ìš°ì„ ìˆœìœ„
if all(k in wide.columns for k in ["ì—…ì¢…","ì§€ì—­"]):
    group_keys = ["ì—…ì¢…","ì§€ì—­"]
elif "ì—…ì¢…" in wide.columns:
    group_keys = ["ì—…ì¢…","íŒŒì¼"]
else:
    group_keys = ["íŒŒì¼"]
groupby_keys = list(dict.fromkeys(group_keys + (["ì—°ë„"] if "ì—°ë„" in wide.columns else [])))
print("\nğŸ§© ê·¸ë£¹ í‚¤(í‘œì‹œìš©):", group_keys, " | ì‹¤ì œ groupby í‚¤:", groupby_keys)

grp = wide.groupby(groupby_keys, as_index=False, sort=False)["ê°€ì¤‘ì‚¬ê±´"].mean()
y_min, y_max = float(grp["ê°€ì¤‘ì‚¬ê±´"].min()), float(grp["ê°€ì¤‘ì‚¬ê±´"].max())
grp["Risk_Score_0_1"] = 0.0 if (y_max-y_min)<1e-12 else (grp["ê°€ì¤‘ì‚¬ê±´"]-y_min)/(y_max-y_min)

def agg_share(df, cols, keys):
    if not cols or not keys: return pd.DataFrame()
    sub = df[keys + cols].copy()
    sub["sum"] = sub[cols].sum(axis=1)
    for c in cols:
        sub[c] = sub[c] / sub["sum"].replace(0, np.nan)
    sub = sub.drop(columns=["sum"])
    return sub.groupby(keys, as_index=False).mean(numeric_only=True)

keys_share = [k for k in ["ì—…ì¢…","ì§€ì—­"] if k in wide.columns]
scale_share = agg_share(wide, scale_cols, keys_share) if keys_share else pd.DataFrame()
tenur_share = agg_share(wide, tenur_cols, keys_share) if keys_share else pd.DataFrame()

# ì €ì¥ë¬¼
risk_lookup_cols = [c for c in ["ì—…ì¢…","ì§€ì—­","íŒŒì¼","ì—°ë„","ê°€ì¤‘ì‚¬ê±´","Risk_Score_0_1"] if c in grp.columns]
grp[risk_lookup_cols].to_csv("risk_lookup.csv", index=False, encoding="utf-8-sig")

feat_keys = [k for k in ["ì—…ì¢…","ì§€ì—­"] if k in grp.columns] or [k for k in ["ì—…ì¢…","íŒŒì¼"] if k in grp.columns] or [k for k in ["íŒŒì¼"] if k in grp.columns]
feat = grp[[c for c in feat_keys + ["ì—°ë„","Risk_Score_0_1"] if c in grp.columns]].copy()
if not scale_share.empty and keys_share:
    feat = feat.merge(scale_share, on=keys_share, how="left")
if not tenur_share.empty and keys_share:
    feat = feat.merge(tenur_share, on=keys_share, how="left")
feat = feat.fillna(0)

print("ğŸ§© feat shape:", feat.shape)
for k in [c for c in feat.columns if c in ["ì—…ì¢…","ì§€ì—­","ì—°ë„","íŒŒì¼"]]:
    try:
        print(f"   - {k} ê³ ìœ ê°’ ìˆ˜:", feat[k].nunique())
    except Exception:
        pass

# â”€â”€ í•™ìŠµ (ì¶©ë¶„í•  ë•Œë§Œ) â”€â”€
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

n_samples = len(feat)
trained = False

if n_samples >= 20:
    cat_cols = [c for c in ["ì—…ì¢…","ì§€ì—­","íŒŒì¼"] if c in feat.columns]
    num_cols = [c for c in feat.columns if c not in (cat_cols + ["Risk_Score_0_1"])]
    df_cat = feat[cat_cols].copy() if cat_cols else pd.DataFrame(index=feat.index)
    X_cat = pd.get_dummies(df_cat) if not df_cat.empty else pd.DataFrame(np.zeros((n_samples,0)))
    X_num = feat[num_cols].copy()
    scaler = StandardScaler()
    if X_num.shape[1]==0:
        X_num = pd.DataFrame(np.zeros((n_samples,1)), columns=["num_dummy"])
    num_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=X_num.columns, index=feat.index)

    X = np.hstack([num_scaled.values, X_cat.values])
    y = feat["Risk_Score_0_1"].astype(float).values

    test_size = 0.2 if n_samples >= 50 else max(1, n_samples//10)/n_samples
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    model = models.Sequential([
        layers.Input(shape=(X_train.shape[1],)),
        layers.Dense(128, activation="relu"),
        layers.Dropout(0.3),
        layers.Dense(64, activation="relu"),
        layers.Dense(1, activation="sigmoid"),
    ])
    model.compile(optimizer="adam", loss="mse", metrics=["mae"])
    es = callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor="val_loss")
    model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=200, batch_size=16, verbose=1, callbacks=[es])
    loss, mae = model.evaluate(X_test, y_test, verbose=0)
    print(f"âœ… í…ŒìŠ¤íŠ¸ MAE: {mae:.4f}")

    model.save("industrial_risk_model.h5"); print("ğŸ§  ì €ì¥: industrial_risk_model.h5")
    with open("scaler.pkl","wb") as f: pickle.dump(scaler,f)
    with open("num_cols.json","w",encoding="utf-8") as f: json.dump({"num_cols": list(num_scaled.columns)}, f, ensure_ascii=False, indent=2)
    with open("onehot_meta.json","w",encoding="utf-8") as f: json.dump({"columns": list(X_cat.columns)}, f, ensure_ascii=False, indent=2)

    feat_out = pd.concat([feat.reset_index(drop=True), pd.DataFrame(X_cat, columns=X_cat.columns)], axis=1)
    feat_out.to_csv("industrial_clean.csv", index=False, encoding="utf-8-sig"); print("ğŸ“„ ì €ì¥: industrial_clean.csv")
    trained = True
else:
    print(f"âš ï¸ í•™ìŠµ ìƒ˜í”Œì´ {n_samples}ê°œë¼ì„œ ML í•™ìŠµ ìŠ¤í‚µ. (ë£°-ê¸°ë°˜ lookupë§Œ ì‚¬ìš©)")

if not os.path.exists("industrial_clean.csv"):
    feat.to_csv("industrial_clean.csv", index=False, encoding="utf-8-sig")

print("\nğŸ‰ ì™„ë£Œ ì‚°ì¶œë¬¼:")
if trained:
    print(" - industrial_risk_model.h5")
    print(" - scaler.pkl")
    print(" - num_cols.json")
    print(" - onehot_meta.json")
print(" - industrial_clean.csv")
print(" - risk_lookup.csv")
if os.path.exists("feature_lookup.csv"):
    print(" - feature_lookup.csv")
